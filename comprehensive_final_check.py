#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import requests
from bs4 import BeautifulSoup
import re

def final_comprehensive_check():
    """Final comprehensive check for English texts in Arabic sidebar"""
    
    session = requests.Session()
    login_url = "http://127.0.0.1:8000/ar/auth/login/"
    dashboard_url = "http://127.0.0.1:8000/ar/"
    
    print("ğŸ” Ø§Ù„ÙØ­Øµ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© ÙÙŠ Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ Ø§Ù„Ø¹Ø±Ø¨ÙŠ")
    print("="*70)
    
    # Step 1: Get login page and extract CSRF
    print("ğŸ“„ Ø§Ù„Ø®Ø·ÙˆØ© 1: Ø¬Ù„Ø¨ ØµÙØ­Ø© ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„...")
    response = session.get(login_url)
    
    if response.status_code != 200:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ÙˆØµÙˆÙ„ Ù„ØµÙØ­Ø© ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„: {response.status_code}")
        return 0
    
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # Find CSRF token from any form (they should all have the same token)
    csrf_input = soup.find('input', {'name': 'csrfmiddlewaretoken'})
    if not csrf_input:
        print("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ CSRF token")
        return 0
    
    csrf_token = csrf_input.get('value')
    print(f"âœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ CSRF token: {csrf_token[:20]}...")
    
    # Step 2: Login
    print("\nğŸ” Ø§Ù„Ø®Ø·ÙˆØ© 2: ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„...")
    login_data = {
        'username': 'super',
        'password': 'password',
        'csrfmiddlewaretoken': csrf_token
    }
    
    response = session.post(login_url, data=login_data, allow_redirects=True)
    print(f"ğŸ“Š Ø§Ø³ØªØ¬Ø§Ø¨Ø© ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„: {response.status_code}")
    print(f"ğŸ”— Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: {response.url}")
    
    # Step 3: Access dashboard
    print("\nğŸ  Ø§Ù„Ø®Ø·ÙˆØ© 3: Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù„ÙˆØ­Ø© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©...")
    response = session.get(dashboard_url)
    
    if response.status_code != 200:
        print(f"âŒ ÙØ´Ù„ Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù„ÙˆØ­Ø© Ø§Ù„Ù‚ÙŠØ§Ø¯Ø©: {response.status_code}")
        return 0
    
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # Check if login was successful
    logout_link = soup.find('a', href=re.compile(r'logout|auth/logout'))
    user_indicator = soup.find(string=re.compile(r'super|admin|Ù…Ø±Ø­Ø¨Ø§Ù‹|Ø£Ù‡Ù„Ø§Ù‹'))
    
    if logout_link or user_indicator:
        print("âœ… ØªÙ… ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¨Ù†Ø¬Ø§Ø­!")
    else:
        print("âŒ ÙØ´Ù„ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„")
        print("ğŸ” Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ø¤Ø´Ø±Ø§Øª Ø£Ø®Ø±Ù‰...")
        # Check page title
        title = soup.find('title')
        if title and 'login' not in title.get_text().lower():
            print("âœ… ØªÙ… ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ (Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ØµÙØ­Ø©)")
        else:
            return 0
    
    # Step 4: Find navigation/sidebar
    print("\nğŸ§­ Ø§Ù„Ø®Ø·ÙˆØ© 4: Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ ÙˆØ§Ù„Ù‚ÙˆØ§Ø¦Ù…...")
    
    # Try multiple methods to find the main navigation
    possible_sidebars = [
        soup.find('nav', class_=re.compile(r'sidebar|side-nav|main-nav')),
        soup.find('div', class_=re.compile(r'sidebar|side-nav|main-nav')),
        soup.find('aside'),
        soup.find(id=re.compile(r'sidebar|navigation|nav')),
        soup.find('ul', class_=re.compile(r'nav|menu')),
    ]
    
    sidebar = None
    for potential_sidebar in possible_sidebars:
        if potential_sidebar and len(potential_sidebar.find_all('a')) >= 5:
            sidebar = potential_sidebar
            break
    
    # If no sidebar found, look for any container with many navigation links
    if not sidebar:
        print("ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£ÙŠ Ø­Ø§ÙˆÙŠ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø±ÙˆØ§Ø¨Ø· ØªÙ†Ù‚Ù„ Ù…ØªØ¹Ø¯Ø¯Ø©...")
        all_elements = soup.find_all(['div', 'nav', 'ul', 'section'])
        
        for element in all_elements:
            links = element.find_all('a')
            # Check if this element has many links that look like navigation
            nav_links = []
            for link in links:
                link_text = link.get_text().strip()
                if link_text and len(link_text) > 2 and len(link_text) < 50:
                    nav_links.append(link)
            
            if len(nav_links) >= 8:  # At least 8 navigation-like links
                sidebar = element
                print(f"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© ØªÙ†Ù‚Ù„ Ù…Ø¹ {len(nav_links)} Ø±Ø§Ø¨Ø·")
                break
    
    if not sidebar:
        print("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø´Ø±ÙŠØ· Ø¬Ø§Ù†Ø¨ÙŠ Ø£Ùˆ Ù‚Ø§Ø¦Ù…Ø© ØªÙ†Ù‚Ù„ Ø±Ø¦ÙŠØ³ÙŠØ©!")
        print("ğŸ” Ø¹Ø±Ø¶ Ø£ÙˆÙ„ 1000 Ø­Ø±Ù Ù…Ù† Ø§Ù„ØµÙØ­Ø© Ù„Ù„ØªØ´Ø®ÙŠØµ:")
        print("-" * 50)
        print(soup.get_text()[:1000])
        print("-" * 50)
        return 0
    
    print("âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©!")
    
    # Step 5: Extract and analyze text
    print("\nğŸ“ Ø§Ù„Ø®Ø·ÙˆØ© 5: ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ ÙÙŠ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø©...")
    
    # Get all text from sidebar
    sidebar_text = sidebar.get_text()
    
    # Find English texts
    english_texts = []
    
    # Method 1: Line by line analysis
    lines = sidebar_text.split('\n')
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # Check if line contains English characters
        if re.search(r'[A-Za-z]', line):
            # Calculate ratio of English characters
            english_chars = len(re.findall(r'[A-Za-z]', line))
            total_chars = len(re.sub(r'\s+', '', line))
            
            if total_chars > 0 and english_chars / total_chars > 0.3:
                english_texts.append(line)
    
    # Method 2: Check individual links
    links = sidebar.find_all('a')
    for link in links:
        link_text = link.get_text().strip()
        if link_text and re.search(r'[A-Za-z]', link_text):
            english_chars = len(re.findall(r'[A-Za-z]', link_text))
            total_chars = len(re.sub(r'\s+', '', link_text))
            
            if total_chars > 0 and english_chars / total_chars > 0.3:
                if link_text not in english_texts:
                    english_texts.append(link_text)
    
    # Remove duplicates and clean up
    english_texts = list(set([text for text in english_texts if len(text.strip()) > 1]))
    english_texts.sort()
    
    # Step 6: Display results
    print(f"\nğŸ“Š Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©:")
    print("="*50)
    print(f"ğŸ”¢ Ø¹Ø¯Ø¯ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ø§Ù„Ù…ÙƒØªØ´ÙØ©: {len(english_texts)}")
    print("-"*30)
    
    if english_texts:
        for i, text in enumerate(english_texts, 1):
            print(f"{i:2d}. {text}")
    else:
        print("ğŸ‰ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØµÙˆØµ Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©! ØªÙ… Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„ØªØ±Ø¬Ù…Ø© Ø¨Ù†Ø³Ø¨Ø© 100%!")
    
    print("-"*30)
    
    # Check specific target texts
    print(f"\nğŸ¯ ÙØ­Øµ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø³ØªÙ‡Ø¯ÙØ©:")
    sidebar_lower = sidebar_text.lower()
    
    bank_accounts_found = 'bank accounts' in sidebar_lower
    global_search_found = 'global search' in sidebar_lower
    
    print(f"- Bank Accounts: {'âŒ Ù…ÙˆØ¬ÙˆØ¯' if bank_accounts_found else 'âœ… Ù…ØªØ±Ø¬Ù…'}")
    print(f"- Global Search: {'âŒ Ù…ÙˆØ¬ÙˆØ¯' if global_search_found else 'âœ… Ù…ØªØ±Ø¬Ù…'}")
    
    # Final status
    if len(english_texts) == 0:
        print(f"\nğŸ‰ ØªÙ‡Ø§Ù†ÙŠÙ†Ø§! ØªÙ… Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„ØªØ±Ø¬Ù…Ø© Ø¨Ù†Ø³Ø¨Ø© 100%!")
        print(f"âœ… Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØµÙˆØµ ÙÙŠ Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø£ØµØ¨Ø­Øª Ù…ØªØ±Ø¬Ù…Ø©!")
    else:
        completion_rate = max(0, 100 - (len(english_texts) * 5))  # Rough calculation
        print(f"\nğŸ“ˆ Ù†Ø³Ø¨Ø© Ø§Ù„Ø¥ÙƒÙ…Ø§Ù„: {completion_rate}%")
        print(f"ğŸ”„ ÙŠØªØ¨Ù‚Ù‰ ØªØ±Ø¬Ù…Ø© {len(english_texts)} Ù†Øµ")
    
    print("="*70)
    print("Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ÙØ­Øµ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ")
    print("="*70)
    
    return len(english_texts)

if __name__ == "__main__":
    final_comprehensive_check()
